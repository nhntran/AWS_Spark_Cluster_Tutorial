{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUTORIALS: SPARK\n",
    "## PART 2: SPARK ON THE STAND ALONE MODE\n",
    "\n",
    "\n",
    "### by Tran Nguyen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1. Introduction](#intro)\n",
    "- [2. Set up Spark Clusters with AWS](#setup)\n",
    "- [3. Submit script on Spark Clusters](#script)\n",
    "- [4. Some AWS CLI basic commands](#awscli)\n",
    "- [5. Use notebook on the Spark clusters](#notebook)\n",
    "- [6. Clean up the resources](#clean_up)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. INTRODUCTION\n",
    "\n",
    "**There are 4 different Spark modes:**\n",
    "1. Local mode: Use Spark on a single machine such as laptop. With this mode, there is not really any distributed computing. This mode is used to learn Spark syntax and prototype the project.\n",
    "The other 3 modes are cluster manager modes:\n",
    "2. Stand alone: working on a cluster.\n",
    "3. YARN\n",
    "4. Mesos\n",
    "YARN and Mesos are used when sharing cluster with a team. \n",
    "\n",
    "**In this tutorial, we will learn about the stand alone mode on AWS which includes:**\n",
    "- Set up the Spark clusters using AWS console and also AWS CLI\n",
    "- Use built-in notebook on the Spark clusters.\n",
    "- Submit Spark scripts to run on the clusters using AWS CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference:\n",
    "Some of the materials are from the Data Engineer nanodegree program on Udacity (https://www.udacity.com/course/data-engineer-nanodegree--nd027)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 2. SET UP SPARK CLUSTERS WITH AWS \n",
    "\n",
    "- A Spark cluster includes multiple machines. In order to use Spark code on each machine, Spark and its dependencies need to be downloaded and installed manually. With EMR, Elastic Map Reduce service from AWS, everything is ready to use. So instead of using EC2, we use the EMR service to set up Spark Clusters.\n",
    "- This tutorial includes 2 approaches setting up using AWS console and CLI. Note that both approaches needs the `Key Pairs` in `EC2 Network & Security`. \n",
    "- EC2 - EMR checklist for the AWS cluster to work (Reference: https://stackoverflow.com/questions/17698876/aws-ssh-access-port-22-operation-timed-out-issue/60134134#60134134):\n",
    "    + On AWS console: is the Instance up and healthy?\n",
    "    + Is it in a public Subnet?\n",
    "    + Does it has a public ip?\n",
    "    + Does the VPC has Internet Gateway?\n",
    "    + Does it has the Routing Table to the Internet Gateway? ( Attached to the subnet?)\n",
    "    + Does the Network ACL rules the default?\n",
    "    + Does the Security group allows ping? If yes, does the ping works?\n",
    "    + Does the Security group allows SSH inbound?\n",
    "    + If there is still no clue, then fire up a new instance (from a base AMI) in the same VPC. Connect to it via SSH. If it was successful, try to ssh from that instance.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up credentials in EC2\n",
    "\n",
    "- From the AWS console, click on `Service`, type 'EC2' to go to EC2 console => Choose `Key Pairs` in **Network & Security** on the left panel => Choose `Create key pair` => Name \"spark-cluster\", File format: pem => the .pem file will be automatically downloaded. Move this .pem file to the desire directory used for the project (usually the root directory)\n",
    "- When using the file to set up the ssh, there would be an error if the .pem file is too open:\n",
    "     + Example: Permissions 0644 for '.aws/spark-cluster.pem' are too open. It is required that your private key files are NOT accessible by others. This private key will be ignored.\n",
    "     + Need to set it using the command:\n",
    "     `sudo chmod 600 .aws/spark-cluster.pem`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. SET UP USING AWS CONSOLE\n",
    "- Tutorials for set up Spark cluster on AWS EMR: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-gs.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- From the AWS console, go to EMR service => Choose `Create cluster` => Name \"spark-udacity\" => Select `emr-5.20.0` release, `Spark:Spark 2.4.5 on Hadoop 2.8.5 YARN and Zeppelin 0.8.2`, instance type `m5.xlarge`, number of instances: `4`, EC2 key pair `spark-cluster` => `Create cluster`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. SET UP USING AWS CLI\n",
    "\n",
    "- CLI: Command Line Interface\n",
    "- Need Python 3.6 or above\n",
    "- Tutorial: https://dziganto.github.io/aws/aws%20cli/emr/big%20data/hadoop/jupyterhub/spark/Setup-an-EMR-Cluster-via-AWS-CLI/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Step 1: Set up credentials using AWS IAM\n",
    "- From the AWS console, click on `Service`, type IAM and go to IAM => Choose `User` => `Add user` => Enter a user name such as 'user_awscli' or 'admin', Choose `Access type` as **Programmatic access**, and then `Next: Permissions`. Click on `Attach existence policies directly` page, choose Set permission as `Administrator Access` and then choose `Next: Tags`. Skip this tag page and choose `Next: Review` => Choose `Create user` => Save the user name, Access Key and Secret Access Key.\n",
    "- Set up the Key Pairs on EC2 and save the .pem file if not done yet.\n",
    "- Create a S3 bucket where you want to store log files, otherwise, a S3 bucket will be automatically created.\n",
    " - From the AWS console, click on `Service`, type 'S3' and go to S3 console => Choose `Create bucket` => Enter a name for the bucket such as 's3-for-emr-cluster'. Keep everything at default to create a bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Step 2: Install `awscli `\n",
    "- Install using `pip install awscli`.\n",
    "- Type `aws help` to check if the installation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Set up `awscli ` environment (create the credentials and config files)\n",
    "##### Approach 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the `credentials` file on terminal as follow (you can use `nano` or any other text editor of your choice):\n",
    "\n",
    "    + Navigate to desire folder, usually the root directory\n",
    "    + `$mkdir .aws` (period to denote a hidden directory)\n",
    "    + `$cd .aws`\n",
    "    + `$nano credentials`\n",
    "    ```\n",
    "    [default]\n",
    "    aws_access_key_id=EXAMPLE_ID\n",
    "    aws_secret_access_key=EXAMPLE_KEY\n",
    "    ```\n",
    "    Use `Ctrl + X`, and then `Y` to save the file and exit nano.\n",
    "- Create the config file:\n",
    "     + `$cat config`\n",
    "    ```\n",
    "    [default]\n",
    "    region=us-west-2\n",
    "    output=json\n",
    "    ```   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 2:\n",
    "- Navigate to desire folder on cli:\n",
    "```\n",
    "$aws configure\n",
    "AWS Access Key ID [None]: Enter your secret key from the user created in Session 2.2.1.\n",
    "AWS Secret Access Key [None]: Enter your secret access key\n",
    "Default region name [None]: us-west-2\n",
    "Default output format [None]: \n",
    "```\n",
    "\n",
    "- The 2 files will be automatically created in the hidden folder .aws, usually located at root directory as follow:\n",
    "\n",
    "    `$cd ~/.aws`\n",
    "    \n",
    "    `$ls` \n",
    "    \n",
    "=> config\t\tcredentials\n",
    "- Check to verify the information in the 2 files:\n",
    "\n",
    "    `$cat credentials`\n",
    "    \n",
    "    `$cat config`    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Create EMR cluster\n",
    "\n",
    "**The command to create emr cluster - Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws emr create-cluster --release-label emr-5.20.0 \n",
    "--instance-groups InstanceGroupType=MASTER,InstanceCount=1,\n",
    "InstanceType=m5.xlarge InstanceGroupType=CORE,InstanceCount=3,InstanceType=m5.xlarge \n",
    "--use-default-roles --ec2-attributes KeyName=YOUR_KEY \n",
    "--applications Name=JupyterHub Name=Spark Name=Hadoop --name=spark-udacity \n",
    "--log-uri s3://spark-test-emr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "- `name`: the name for the cluster, such as 'test-emr-cluster'\n",
    "- `--release-label emr-5.20.0`: build a cluster with EMR version 5.20.0\n",
    "\n",
    "- `--instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m5.xlarge InstanceGroupType=CORE,InstanceCount=3,InstanceType=m5.xlarge`: build 1 Master node of type m5.xlarge and 3 Core nodes also of type m5.xlarge\n",
    "\n",
    "- `--use-default-roles`: use the default service role (EMR_DefaultRole) and instance profile (EMR_EC2_DefaultRole) for permissions to access other AWS services\n",
    "\n",
    "- `--ec2-attributes KeyName=YOUR_KEY`: configures Amazon EC2 instance configurations, the EC2 instance name that we set up in step `2.2.2. Set up credentials in EC2` and get the .pem file. \n",
    "\n",
    "- `--applications Name=JupyterHub Name=Spark Name=Hadoop`: install JupyterHub, Spark, and Hadoop on this cluster\n",
    "\n",
    "- `--name=spark-udacity`: name the cluster, for example `spark-udacity` \n",
    "\n",
    "- `--log-uri s3://spark-test-emr`: specify the S3 bucket where you want to store log files \n",
    "\n",
    "- `--auto-terminate`: EMR clusters are costly => auto-terminate when done. When you use auto-termination, the cluster starts, runs any bootstrap actions that you specify, and then executes steps that typically input data, process the data, and then produce and save output. When the steps finish, Amazon EMR automatically terminates the cluster Amazon EC2 instances. If we don't put any bootstrap action, we should remove this field to access the ssh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The command to create emr cluster - Example 2:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws emr create-cluster --name spark-udacity \n",
    "--use-default-roles \n",
    "--release-label emr-5.28.0 --instance-count 3 \n",
    "--applications Name=Spark \n",
    "--ec2-attributes KeyName=spark-cluster \n",
    "--instance-type m5.xlarge \n",
    "--log-uri s3:///s3-for-emr-cluster/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- `--release-label emr-5.20.0`: build a cluster with EMR version 5.20.0\n",
    "\n",
    "- `--instance-count 3` and`--instance-type m5.xlarge`: build 1 Master node and 2 Core nodes of type m5.xlarge\n",
    "\n",
    "- `--use-default-roles`: use the default service role (EMR_DefaultRole) and instance profile (EMR_EC2_DefaultRole) for permissions to access other AWS services\n",
    "\n",
    "- `--ec2-attributes KeyName=YOUR_KEY`: the EC2 instance name that we set up in step `2.2.2. Set up credentials in EC2` and get the .pem file. In this case, the name is `spark-cluster`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The command to create emr cluster - Example 3:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws emr create-cluster --name test-emr-cluster \n",
    "--use-default-roles --release-label emr-5.28.0 \n",
    "--instance-count 3 --instance-type m5.xlarge \n",
    "--applications Name=JupyterHub Name=Spark Name=Hadoop \n",
    "--ec2-attributes KeyName=emr-cluster  \n",
    "--log-uri s3://s3-for-emr-cluster/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EMR Script Components:** \n",
    "- `--name`: the name for the cluster, in this case is 'test-emr-cluster'\n",
    "- `--use-default-roles`: use the default service role (EMR_DefaultRole) and instance profile (EMR_EC2_DefaultRole) for permissions to access other AWS services\n",
    "- `--release-label emr-5.28.0`: build a cluster with EMR version 5.28.0\n",
    "- `--instance-count 3` and`--instance-type m5.xlarge`: build 1 Master node and 2 Core nodes of type m5.xlarge\n",
    "- `--applications Name=JupyterHub Name=Spark Name=Hadoop`: install JupyterHub, Spark, and Hadoop on this cluster\n",
    "- `--ec2-attributes KeyName=emr-cluster`: configures Amazon EC2 instance configurations, KeyName is the EC2 instance name that we set up in step 3 `Set up credentials in EC2` and get the .pem file. In this case, the name is `emr-cluster`. We should provide a specific subnet and key here\n",
    "- `--log-uri s3://s3-for-emr-cluster/`: specify the S3 bucket where you want to store log files. In this case, the S3 bucket is 's3-for-emr-cluster'. This optional.\n",
    "- Since EMR clusters are costly, we can put the option `--auto-terminate` to auto-terminate the cluster when all the actions on the cluster are done. For this to work, we also need to specify the boostrap action in the command using `--bootstrap-actions Path=\"s3://bootstrap.sh\"`. When you use auto-termination, the cluster starts, runs any bootstrap actions that you specify, and then executes steps that typically input data, process the data, and then produce and save output. When the steps finish, Amazon EMR automatically terminates the cluster Amazon EC2 instances. If we don't put any bootstrap action, we should remove this field. The reference for boostrap could be found in details on AWS site: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desired Output:**\n",
    "\n",
    "\n",
    "{\n",
    "    \"ClusterId\": \"j-xxxxxxxxxxxxx\",\n",
    "    \"ClusterArn\": \"arn:aws:elasticmapreduce:us-west-2:xxxxxxxxxxxx:cluster/j-xxxxxxxxxxxxx\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Check it on AWS EMR console or check using the command:**\n",
    "\n",
    "`aws emr describe-cluster --cluster-id j-xxxxxxxxxxxxx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5. Allow SSH Access\n",
    "- Reference: https://docs.amazonaws.cn/en_us/emr/latest/ManagementGuide/emr-gs-ssh.html\n",
    "\n",
    "- From the AWS console, click on `Service`, type EMR and go to EMR console => Choose `Clusters` => Choose the name of the cluster on the list. In this case `test-emr-cluster` => On the `Summary` tab, scroll down to see the part `Security and access`, choose the `Security groups for Master` link => Choose the `Security group ID` for `ElasticMapReduce-master` => Scroll down on `Inbound rules`, `Edit inbound rules` => Delete any SSH rule if have, then choose `Add Rule` => Choose Type: SSH, TCP for Protocol and 22 for Port Range => For source, select My IP => `Save` => We will use that IP for ssh command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.6.Create an SSH connection with the master node of the cluster\n",
    "\n",
    "- Reference: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-connect-master-node-ssh.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 1:\n",
    "- On the terminal, check the id of the cluster using the command\n",
    "\n",
    "`aws emr list-clusters`\n",
    "- Type on the terminal\n",
    "\n",
    "`aws emr ssh --cluster-id j-xxxxxxxxxxxx --key-pair-file .aws/spark-cluster.pem`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 2:\n",
    "- From the AWS console, click on `Service`, type EMR and go to EMR console => Choose `Clusters` => Choose the name of the cluster on the list. In this case `test-emr-cluster` => On the Summary tab, Click the link **Connect to the Master Node Using SSH**:\n",
    "\n",
    "<img src=\"img/ssh2.png\" width=\"80%\"/>\n",
    "\n",
    "- Copy and paste on the terminal the command shown on the pop-up window:\n",
    "\n",
    "<img src=\"img/ssh2_link.png\" width=\"80%\"/>\n",
    "\n",
    "- Replace ~/emr-cluster.pem with the location and filename of the private key file (.pem) we have set up.\n",
    "\n",
    "`ssh -i ~/.aws/emr-cluster.pem hadoop@ec2-xx-xxx-xx-xxx.us-west-2.compute.amazonaws.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='script'></a>\n",
    "## 3. SUBMIT SCRIPT ON THE SPARK CLUSTER\n",
    "\n",
    "- After connecting to the spark cluster using ssh, we can submit the python script to the spark cluster.\n",
    "- Note that when the EMR instance is terminated, everything on the EMR instance will be lost. It is the best practice to save the initial, final, and intermediate data of the data pipeline in the S3 for future retrieval. It is best practice to move your files from your local machine to AWS S3, then use the program to read the data from AWS S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. WRITE A SCRIPT AND SUBMIT DIRECTLY ON THE SPARK CLUSTER\n",
    "\n",
    "- Write a simple script in the cluster:\n",
    "\n",
    "`nano test_emr.py`\n",
    "\n",
    "- Finding location of spark-submit:\n",
    "\n",
    "`which spark-submit`\n",
    "\n",
    "- Submit the script:\n",
    "\n",
    "`spark-submit --master yarn ./test_emr.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip running this cell\n"
     ]
    }
   ],
   "source": [
    "%%script echo Skip running this cell\n",
    "### Content of the file 'test_emr.py'\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "       \texample of script submited to spark cluster\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    df = spark.createDataFrame([('01/Jul/1995:00:00:01 -0400', ),('01/Jul/1995:00:00:11 -0400',),('26/Jul/1995:17:07:52 -0400',)\n",
    "                            ('19/Jul/1995:01:56:43 -0400',), ('11/Jul/1995:12:50:18 -0400',)], ['TIME'])\n",
    "    df = df.withColumn(\"date\", F.unix_timestamp(F.col('TIME'), 'dd/MMM/yyyy:HH:mm:ss Z').cast('timestamp'))\n",
    "    # show the dataframe\n",
    "    df.show()\n",
    "    \n",
    "    #### stop the spark, otherwise the program will be hanged => Good for streamming\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. COPY A SCRIPT FILE TO THE SPARK CLUSTER\n",
    "\n",
    "- Create the folder on the spark cluster:\n",
    "\n",
    "    [hadoop@ip-xxx-xx-xx-xxx ~] hdfs dfs -mkdir /user/my_folder_name\n",
    "    \n",
    "- Copy the file to the `my_folder_name` on the spark cluster:\n",
    "    + Copy on the local machine: On terminal of the local machine, using the command scp\n",
    "\n",
    "        `scp test_emr.py hadoop@<YOUR IP>:/home/hadoop/`\n",
    "\n",
    "\n",
    "        `scp test_emr.py hadoop@ec2-xx-xxx-xx-xxx.us-west-2.compute.amazonaws.com:~/`\n",
    "\n",
    "        `scp test_emr.py -i .aws/spark-cluster.pem hadoop@ec2-xx-xxx-xx-xxx.us-west-2.compute.amazonaws.com:~/`\n",
    "    \n",
    "    + Copy on the spark cluster:\n",
    "    \n",
    "        [hadoop@ip-xxx-xx-xx-xxx ~] hdfs dfs -copyFromLocal  test_emr.py /user/my_folder_name/\n",
    "        \n",
    "- Finding location of spark-submit and submit the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='awscli'></a>\n",
    "## 4. SOME OTHER AWS CLI BASIC COMMANDS\n",
    "\n",
    "- Type `aws help` to check for specific commands.\n",
    "- Type `aws s3 help` to check for specific commands in s3.\n",
    "- Some aws commands:\n",
    "\n",
    "    + List all IAM user: `aws iam list-users` \n",
    "    + List all buckets in s3: `aws s3 ls`\n",
    "- More could be found in here: https://www.thegeekstuff.com/2019/04/aws-s3-cli-examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Copy file from local to a s3 bucket\n",
    "    \n",
    "- `aws s3 cp <your current file location>/<filename> s3://<bucket_name>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Copy file to HDFS on the Spark cluster\n",
    "\n",
    "- Create the folder on the spark cluster:\n",
    "\n",
    "`[hadoop@ip-xxx-xx-xx-xxx ~] hdfs dfs -mkdir /user/sparkify_data`\n",
    "\n",
    "or an older but still working version:\n",
    "\n",
    "`[hadoop@ip-xxx-xx-xx-xxx ~] hadoop fs -mkdir /user/sparkify_data`\n",
    "\n",
    "- Copy file from local to the spark cluster:\n",
    "    \n",
    "`[hadoop@ip-xxx-xx-xx-xxx ~] hdfs dfs -copyFromLocal  sparkify_log_small_2.json /user/sparkify_data/`\n",
    "\n",
    "- Verify by using the command:\n",
    "`[hadoop@ip-xxx-xx-xx-xxx ~] hdfs dfs -mkdir /user/sparkify_data`\n",
    "\n",
    "- **Look for the HDFS files on the AWS console:**\n",
    "On AWS EMR console, choose the cluster. On the Summary tab, click on **(View All)** on **Connections:** => Click on the URL of the **HDFS Name Node** => Choose the tab **Utilities** => **Browse the file system**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='notebook'></a>\n",
    "## 5. USE NOTEBOOK ON THE SPARK CLUSTER\n",
    "\n",
    "- On AWS EMR console, notebook can be created and opened in a cluster once the cluster is in Waiting or Running status. In another approach, notebook could be created, and during this creating process, a cluster is created for that notebook.\n",
    "- In EMR, the EMR cluster and EMR notebook are decoupled; notebook could be reattached to a different cluster.\n",
    "- There are 2 types of notebook: Jupyter Notebook & Zeppelin notebooks. \n",
    "- Zeppelin has been available since EMR 5.x versions, and they have direct access to Spark Context, such as a local spark-shell. For example, `sc` is already defined as Spark Context within Zeppelin notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Simple Spark job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip running this cell\n"
     ]
    }
   ],
   "source": [
    "%%script echo Skip running this cell\n",
    "#### CONTENT FOR THE NOTEBOOK\n",
    "\n",
    "log_of_songs = [\n",
    "        \"Despacito\",\n",
    "        \"Nice for what\",\n",
    "        \"No tears left to cry\",\n",
    "        \"Despacito\",\n",
    "        \"Havana\",\n",
    "        \"In my feelings\",\n",
    "        \"Nice for what\",\n",
    "        \"despacito\",\n",
    "        \"All the stars\"\n",
    "]\n",
    "\n",
    "distributed_song_log = sc.parallelize(log_of_songs)\n",
    "distributed_song_log.map(lambda x: x.lower()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Read & write data to S3\n",
    "\n",
    "- A s3 path has 2 parts: the bucket and the key for the object. For example, in the path `s3://my_bucket/path_to_file/file.csv`:\n",
    "    + `s3://my_bucket` is the bucket\n",
    "    + `path_to_file/file.csv` is the key\n",
    "- Read a file in spark: \n",
    "\n",
    "        `df = spark.read.load(“s3://my_bucket/path_to_file/file.csv”)`\n",
    "- If all the files underneath `my_bucket` have the same schema, spark can generate a dataframe for all the files.\n",
    "- If there are conflicts in schema between files, then the dataframe will not be generated.\n",
    "- For example an s3 bucket `my_bucket` has all the files with the same schema like this:\n",
    "            my_bucket\n",
    "              |---test.csv\n",
    "              path/to/\n",
    "                 |--test2.csv\n",
    "                 file/\n",
    "                   |--test3.csv\n",
    "                   |--file.csv\n",
    "we can read all the files using: `df = spark.read.load(“s3://my_bucket/”)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CONTENT FOR THE NOTEBOOK\n",
    "\n",
    "### Copy some data to s3 as in Session 4.1\n",
    "sparkify_log_data = \"s3n://sparkify/sparkify_log_small.json\"\n",
    "\n",
    "df = spark.read.json(sparkify_log_data)\n",
    "df.persist()\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Read & write to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CONTENT FOR THE NOTEBOOK\n",
    "\n",
    "### Copy some data to hdfs as in Session 4.2\n",
    "sparkify_log2_path = \"hdfs:///user/sparkify_data/sparkify_log_small_2.json\"\n",
    "\n",
    "df2 = spark.read.json(sparkify_log2_path)\n",
    "df2.persist()\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean_up'></a>\n",
    "## 6. CLEAN UP THE RESOURCES\n",
    "\n",
    "DO NOT RUN THIS UNLESS YOU ARE SURE TO DELETE YOUR CLUSTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Type `$logout` to log out of the cluster. \n",
    "\n",
    "- Then terminate the cluster using this command:\n",
    "\n",
    "    `$aws emr terminate-clusters --cluster-id j-EXAMPLECLUSTERID`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference for `\"EMR_DefaultRole is invalid\" or \"EMR_EC2_DefaultRole is invalid\" error` when creating an Amazon EMR cluster: https://aws.amazon.com/premiumsupport/knowledge-center/emr-default-role-invalid/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
